{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p>How To Guides, Best Practices and Tutorial Pages:</p> <pre><code>- How To: Create UCSC Tracks\n- How To: Setup HPCDME\n- How To, Best Practoces: GitHub\n- Tutorial: Snakemake\n</code></pre> <p>The DEVIL is in the DETAILS. </p> <p></p>"},{"location":"GitHub/overview/","title":"Overview of GitHub Topics","text":"<ol> <li>Preparing your environment:<ul> <li>Describes how to create a PAT, add GH to your bash profile and use password-less login features</li> </ul> </li> <li>Creating your GitHub repo:<ul> <li>Provides information on how to setup a GitHub repository under CCBR, use of templates, and basic GitHub commands</li> </ul> </li> <li>Repository Management:<ul> <li>Best practices surrounding how to create a new repository within CCBR</li> </ul> </li> <li>Documentation:<ul> <li>Best practices surrounding how to create GitHub Pages documentation for pipelines</li> </ul> </li> <li>GitHub Actions:<ul> <li>Best practices for using GitHub Actions provided with template repos</li> </ul> </li> <li>Test Data:<ul> <li>Best practices for generating test data, accompanying manifests, and required source documentation</li> </ul> </li> </ol>"},{"location":"GitHub/setup_env/","title":"GitHub Setup: Preparing the Environment","text":""},{"location":"GitHub/setup_env/#using-github-cli","title":"Using GitHub CLI","text":"<p>The <code>gh</code> is installed on Biowulf at <code>/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh</code>. You can run the following lines to edit your <code>~/.bashrc</code> file to add <code>gh</code> to your <code>$PATH</code>: <pre><code>echo \"export PATH=$PATH:/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre></p> <p>Alternatively, you can use the <code>git</code> commands provided through a Biowulf module <pre><code>module load git\n</code></pre></p>"},{"location":"GitHub/setup_env/#creating-pat-for-gh","title":"Creating PAT for GH","text":"<p>Personal Access Token (PAT) is required to access GitHub (GH) without having to authenticate by other means (like password) every single time. You will need gh cli installed on your laptop or use <code>/data/CCBR_Pipeliner/db/PipeDB/bin/gh_1.7.0_linux_amd64/bin/gh</code> on biowulf, as described above. You can create a PAT by going here. Then you can copy the PAT and save it into a file on biowulf (say <code>~/gh_token</code>). Next, you can run the following command to set everything up correctly on biowulf (or your laptop) <pre><code>gh auth login --with-token &lt; ~/git_token\n</code></pre></p>"},{"location":"GitHub/setup_env/#password-less-login","title":"Password-less Login","text":"<p>If you hate to re-enter (username and) password everytime you push/pull to/from github (or mkdocs gh-deploy), then it is totally worthwhile to spend a couple minutes to set up SSH keys for auto-authentication. The instructions to do this are available here.</p>"},{"location":"GitHub/setup_repo/","title":"GitHub Setup: Repository!","text":""},{"location":"GitHub/setup_repo/#repository-location","title":"Repository Location","text":"<ul> <li>All CCBR developed pipelines should be created under CCBR's GitHub Org Account.</li> </ul>"},{"location":"GitHub/setup_repo/#use-of-cookiecutter-templates","title":"Use of CookieCutter Templates","text":"<ul> <li>All CCBR developed pipelines should be created from the appropriate cookiecutter template:<ul> <li>TechDev Projects: https://github.com/CCBR/CCBR_CCBRTechDevCookieCutter</li> <li>Snakemake Pipelines: https://github.com/CCBR/CCBR_SnakemakePipelineCookiecutter</li> </ul> </li> </ul>"},{"location":"GitHub/setup_repo/#creating-a-new-repository","title":"Creating a new repository","text":"<p>To create a new repository on Github using gh cli, you can run the following command on Biowulf after you update the new repository name (<code>&lt;ADD NEW REPO NAME&gt;</code>) and the repository description (<code>&lt;ADD REPO DESCRIPTION&gt;</code>) commands below. </p> <p>NOTE: Do not remove the <code>CCBR/</code> leading the repository name, as this will correctly place the repository under the CCBR organizaiton account.</p> <pre><code>gh repo create CCBR/&lt;ADD NEW REPO NAME&gt; \\\n--template CCBR/CCBR_SnakemakePipelineCookiecutter \\\n--description \"&lt;ADD REPO DESCRIPTION&gt;\" \\\n--public \\\n--confirm\n</code></pre> <p>Once the repo is created, then you can clone a local copy of the new repository:</p> <pre><code>gh repo clone CCBR/&lt;reponame&gt;.git\n</code></pre>"},{"location":"GitHub/setup_repo/#github-functions","title":"GitHub Functions","text":"<ul> <li>The following outlines basic GitHub function to <code>push</code> and <code>pull</code> from your repository. It also includes information on creating a new branch and deleting a branch. These commands should be used in line with guidance on GitHub Repo Management.</li> </ul>"},{"location":"GitHub/setup_repo/#pushing-local-changes-to-remote","title":"Pushing local changes to remote","text":"<p>Check which files have been changed.</p> <pre><code>git status\n</code></pre> <p>Stage files that need to be pushed <pre><code>git add &lt;thisfile&gt;\ngit add &lt;thatfile&gt;\n</code></pre></p> <p>Push changes to branch named <code>new_feature</code> <pre><code>git push origin new_feature\n</code></pre></p>"},{"location":"GitHub/setup_repo/#pulling-remote-changes-to-local","title":"Pulling remote changes to local","text":"<p>Pull changes from branch <code>new_feature</code> into your branch <code>old_feature</code> <pre><code>git checkout old_feature\ngit pull new_feature\n</code></pre></p> <p>If you have non-compatable changes in the <code>old_feature</code> branch, there are two options: 1) ignore local changes and pull remote anyways. This will delete the changes you've made to your remote respository. <pre><code>git reset --hard\ngit pull\n</code></pre> 2) temporarily stash changes away, pull and reapply changes after. <pre><code>git stash\ngit pull\ngit stash pop\n</code></pre></p>"},{"location":"GitHub/setup_repo/#creating-a-new-branch","title":"Creating a new branch","text":"<p>This is a two step process.</p> <ul> <li> <p>Create the branch locally <pre><code>git checkout -b &lt;newbranch&gt;\n</code></pre></p> </li> <li> <p>Push the branch to remote <pre><code>git push -u origin &lt;newbranch&gt;\n</code></pre> OR <pre><code>git push -u origin HEAD\n</code></pre> This is a shortcut to push the current branch to a branch of the same name on <code>origin</code> and track it so that you don't need to specify <code>origin HEAD</code> in the future.</p> </li> </ul>"},{"location":"GitHub/setup_repo/#deleting-branches","title":"Deleting branches","text":""},{"location":"GitHub/setup_repo/#locally","title":"Locally","text":"<pre><code>git branch -d &lt;BranchName&gt;\n</code></pre>"},{"location":"GitHub/setup_repo/#on-github","title":"on GitHub","text":"<pre><code>git push origin --delete &lt;BranchName&gt;\n</code></pre>"},{"location":"GitHub/sop_actions/","title":"GitHub Best Practices: GitHub Actions","text":"<p>The following describe the minimum GitHub actions that should be deployed with any production pipeline. The actions are automatically provided via the cookiecutter template.</p> <ol> <li> <p>Documentation (assumes <code>mkdocs build</code>; required)</p> <ul> <li>These rules will automatically update any documentation built with mkdocs for all PR's.</li> <li>Rule Name(s): mkdocs_build \u2192 pages-build-and-deployment</li> </ul> </li> <li> <p>Dry-run with test sample data for any PR to dev branch (required)</p> <ul> <li>This rule will automatically perform a dry-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.</li> </ul> </li> <li> <p>Full-run with full sample data for any PR to main branch (required)</p> <ul> <li>This rule will automatically perform a full-run with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.</li> </ul> </li> <li> <p>Lintr (required)</p> <ul> <li>This rule will automatically perform a lintr with the data provided in the .test folder of the pipeline. Review the GitHub Best Practices - Test Data page for more information.</li> </ul> </li> <li> <p>Auto pull/push from source (if applicable)</p> <ul> <li>If the pipeline is forked from another location and updating this forked pipeline is required, an action will automatically perform a pull from the source location at least once a week.</li> </ul> </li> </ol>"},{"location":"GitHub/sop_docs/","title":"GitHub Best Practices: Documentation","text":"<p>GitHub Pages is quick and easy way to build static websites for your GitHub repositories. Essentially, you write pages in Markdown which are then rendered to HTML and hosted on GitHub, free of cost! </p> <p>CCBR has used GitHub pages to provide extensive, legible and origanized documentation for our pipelines. Examples are included below:</p> <ul> <li>CARLISLE</li> <li>Pipeliner</li> <li>RNA-seek</li> </ul> <p><code>Mkdocs</code> is the with documentation tool prefered, with the Material theme, for most of the CCBR GitHub Pages websites.</p>"},{"location":"GitHub/sop_docs/#install-mkdocs-themes","title":"Install MkDocs, themes","text":"<p><code>mkdocs</code> and the Material for mkdocs theme can be installed using the following:</p> <pre><code>pip install --upgrade pip\npip install mkdocs\npip install mkdocs-material\n</code></pre> <p>Also install other common dependencies:</p> <pre><code>pip install mkdocs-pymdownx-material-extras\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-revision-date-plugin\npip install mkdocs-minify-plugin\n</code></pre>"},{"location":"GitHub/sop_docs/#conventions","title":"Conventions","text":"<p>Generally, for GitHub repos with GitHub pages:</p> <ul> <li>The repository needs to be public (not private)</li> <li>The main/master branch has the markdown documents under a <code>docs</code> folder at the root level</li> <li>Rendered HTMLs are hosted under a <code>gh-pages</code> branch at root level</li> </ul>"},{"location":"GitHub/sop_docs/#create-website","title":"Create website","text":"<p>The following steps can be followed to build your first website </p>"},{"location":"GitHub/sop_docs/#add-mkdocsyaml","title":"Add <code>mkdocs.yaml</code>","text":"<p><code>mkdocs.yaml</code> needs to be added to the root of the master branch. A template of this file is available in the cookiecutter template.</p> <pre><code>git clone https://github.com/CCBR/xyz.git\ncd xyz\nvi mkdocs.yaml\ngit add mkdocs.yaml\ngit commit -m \"adding mkdocs.yaml\"\ngit push\n</code></pre> <p>Here is a sample <code>mkdocs.yaml</code>:</p> <pre><code># Project Information\nsite_name: CCBR How Tos\nsite_author: Vishal Koparde, Ph.D.\nsite_description: &gt;-\nThe **DEVIL** is in the **DETAILS**. Step-by-step detailed How To Guides for data management and other CCBR-relevant tasks.\n\n# Repository\nrepo_name: CCBR/HowTos\nrepo_url: https://github.com/CCBR/HowTos\nedit_uri: https://github.com/CCBR/HowTos/edit/main/docs/\n\n# Copyright\ncopyright: Copyright &amp;copy; 2023 CCBR\n\n# Configuration\ntheme:\nname: material\nfeatures:\n- navigation.tabs\n- navigation.top\n- navigation.indexes\n- toc.integrate palette:\n- scheme: default\nprimary: indigo\naccent: indigo\ntoggle:\nicon: material/toggle-switch-off-outline\nname: Switch to dark mode\n- scheme: slate\nprimary: red\naccent: red\ntoggle:\nicon: material/toggle-switch\nname: Switch to light mode\nlogo: assets/images/doc-book.svg\nfavicon: assets/images/favicon.png\n\n# Plugins\nplugins:\n- search\n- git-revision-date\n- minify:\nminify_html: true\n\n\n# Customization\nextra:\nsocial:\n- icon: fontawesome/solid/users\nlink: http://bioinformatics.cancer.gov\n- icon: fontawesome/brands/github\nlink: https://github.com/CCRGeneticsBranch\n- icon: fontawesome/brands/docker\nlink: https://hub.docker.com/orgs/nciccbr/repositories\nversion:\nprovider: mike\n\n\n# Extensions\nmarkdown_extensions:\n- markdown.extensions.admonition\n- markdown.extensions.attr_list\n- markdown.extensions.def_list\n- markdown.extensions.footnotes\n- markdown.extensions.meta\n- markdown.extensions.toc:\npermalink: true\n- pymdownx.arithmatex:\ngeneric: true\n- pymdownx.betterem:\nsmart_enable: all\n- pymdownx.caret\n- pymdownx.critic\n- pymdownx.details\n- pymdownx.emoji:\nemoji_index: !!python/name:materialx.emoji.twemoji\nemoji_generator: !!python/name:materialx.emoji.to_svg\n- pymdownx.highlight\n- pymdownx.inlinehilite\n- pymdownx.keys\n- pymdownx.magiclink:\nrepo_url_shorthand: true\nuser: squidfunk\nrepo: mkdocs-material\n- pymdownx.mark\n- pymdownx.smartsymbols\n- pymdownx.snippets:\ncheck_paths: true\n- pymdownx.superfences\n- pymdownx.tabbed\n- pymdownx.tasklist:\ncustom_checkbox: true\n- pymdownx.tilde\n\n# Page Tree\nnav:\n- Intro : index.md\n</code></pre>"},{"location":"GitHub/sop_docs/#create-indexmd","title":"Create <code>index.md</code>","text":"<p>Create <code>docs</code> folder, add your <code>index.md</code> there.</p> <pre><code>mkdir docs\necho \"### Testing\" &gt; docs/index.md\ngit add docs/index.md\ngit commit -m \"adding landing page\"\ngit push\n</code></pre>"},{"location":"GitHub/sop_docs/#build-site","title":"Build site","text":"<p><code>mkdocs</code> can now be used to render <code>.md</code> to HTML</p> <pre><code>mkdocs build\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/$USER/Documents/GitRepos/parkit/site\nINFO     -  Documentation built in 0.32 seconds\n</code></pre> <p>The above command creates a local <code>site</code> folder which is the root of your \"to-be-hosted\" website. You can now open the HTMLs in the <code>site</code> folder locally to ensure that that HTML is as per you liking. If not, then you can make edits to the <code>.md</code> files and rebuild the site.</p> <p>NOTE: You do not want to push the <code>site</code> folder back to GH and hence it needs to be added to <code>.gitignore</code> file:</p> <pre><code>echo \"**/site/*\" &gt; .gitignore\ngit add .gitignore\ngit commit -m \"adding .gitignore\"\ngit push\n</code></pre>"},{"location":"GitHub/sop_docs/#deploy-site","title":"Deploy site","text":"<p>The following command with auto-create a <code>gh-pages</code> branch (if it does not exist) and copy the contents of the <code>site</code> folder to the root of that branch. It will also provide you the URL to your newly created website.</p> <pre><code>mkdocs gh-deploy\nINFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /Users/kopardevn/Documents/GitRepos/xyz/site\nINFO     -  Documentation built in 0.34 seconds\nWARNING  -  Version check skipped: No version specified in previous deployment.\nINFO     -  Copying '/Users/kopardevn/Documents/GitRepos/xyz/site' to 'gh-pages' branch and pushing to\n            GitHub.\nEnumerating objects: 51, done.\nCounting objects: 100(51/51), done.\nDelta compression using up to 16 threads\nCompressing objects: 100(47/47), done.\nWriting objects: 100(51/51), 441.71 KiB | 4.29 MiB/s, done.\nTotal 51 (delta 4), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100(4/4), done.\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/CCBR/xyz/pull/new/gh-pages\nremote:\nTo https://github.com/CCBR/xyz.git\n * [new branch]      gh-pages -&gt; gh-pages\nINFO     -  Your documentation should shortly be available at: https://CCBR.github.io/xyz/\n</code></pre> <p>Now if you point your web browser to the URL from <code>gh-deploy</code> command (IE https://CCBR.github.io/xyz/) you will see your HTML hosted on GitHub. After creating your docs, the cookiecutter template includes a GitHub action which will automatically perform the above tasks whenever a push is performed to the main branch.</p>"},{"location":"GitHub/sop_docs/#add-to-the-github-page","title":"Add to the GitHub page","text":"<ol> <li>Go to the main GitHub page of your repository</li> <li>On the top right select the <code>gear</code> icon next to <code>About</code></li> <li>Under <code>Website</code>, select <code>Use your GitHub Pages website</code>.</li> <li>Select <code>Save Changes</code></li> </ol>"},{"location":"GitHub/sop_repo/","title":"GitHub Best Practices: Repository Management","text":"<p>Users should follow the set-up information provided to learn more about where and how to create a new repository, before following the management best practices below.</p>"},{"location":"GitHub/sop_repo/#security-settings","title":"Security settings","text":"<ul> <li>Two members of CCBR (creator and one manager) should be granted full administrative priveleges to the repository to ensure the source code can be accessed by other members, as needed</li> <li>Both the develop and master branch must be protected (IE have to have a PR to be changed) </li> </ul>"},{"location":"GitHub/sop_repo/#ccbr-branch-strategy","title":"CCBR Branch Strategy","text":""},{"location":"GitHub/sop_repo/#branch-overview","title":"Branch Overview","text":"<ul> <li>All repositories should include a minimum of two branches at any time: <ul> <li>main (master) </li> <li>dev</li> </ul> </li> <li>Additional bracnhes should be created as needed. These would include feature branches, developed using individual, feature specific addition and hotfix branches, developed using individual, bug specific fixes. </li> <li>Utilization of these branches should follow the documentation below.</li> </ul>"},{"location":"GitHub/sop_repo/#strategy-outline","title":"Strategy Outline","text":"<p>We encourage the use of the Git Flow tools for some actions, available on Biowulf. Our current branching strategy is based off of the Git Flow strategy shown below :</p> <p></p> <p>The CCBR Branching strategy is as follows:</p> <ol> <li>Master (named main or master)<ul> <li>branch that contains the current release / tagged version of the pipeline</li> <li>merges from Dev branch or hotfix branch allowed</li> <li>merges require actions_master_branch <code>pass</code> from GitHub actions. See GitHub actions #4 for more information testing requirements for merge</li> </ul> </li> <li>Develop (named dev or activeDev)<ul> <li>branch that contains current dev</li> <li>merges from feature branch allowed</li> <li>merges require actions_dev_branch <code>pass</code> from GitHub actions. See GitHub actions #3 for more information testing requirements for merge</li> </ul> </li> <li>Feature (named feature/unique_feature_name)<ul> <li>branch to develop new features that branches off the develop branch</li> <li>recommended usages of <code>git flow feature start unique_feature_name</code></li> <li>no merges into this branch are expected</li> </ul> </li> <li>Hotfix (named unique_hotfix_name)<ul> <li>branches arise from a bug that has been discovered and must be resolved; it enables developers to keep working on their own changes on the develop branch while the bug is being fixed</li> <li>recommended usage of <code>git flow hotfix start unique_hotfix_name</code></li> <li>no merges into this branch are expected </li> </ul> </li> </ol>"},{"location":"GitHub/sop_repo/#note","title":"Note","text":"<ul> <li>While the <code>git flow feature start</code> command is recommended for feature branch merging, the <code>git flow feature finish</code> is not. Using the <code>finish</code> command will automatically merge the <code>feature</code> branch into the <code>dev</code> branch, without any testing, and regardless of divergence that may have occured during feature development.</li> </ul>"},{"location":"GitHub/sop_repo/#release-tagged-nomenclature","title":"Release, Tagged Nomenclature","text":"<p>The following format of versioning should be followed:</p> <pre><code>v.X.Y.Z\n</code></pre> <p>The following rules should be applies when determining the version release:</p> <ul> <li>X is major; non-backward compatible (dependent on the amount of changes; from dev) </li> <li>Y is minor; backwards compatible (dependent on the amount of changes; from dev) </li> <li>Z is patches; backwards compatible (bugs; hot fixes) </li> </ul> <p>Other notes:</p> <ul> <li>X,Y,Z must be numeric only</li> <li>All updates to the main (master) branch must be tagged and versioned using the parameters above</li> <li>Updates to the dev branch can be tagged, but should not be versioned</li> <li>If the pipeline is available locally (IE on Biowulf), version changes should be added for use</li> </ul>"},{"location":"GitHub/sop_repo/#documentation","title":"Documentation","text":"<ul> <li>All pipelines should provide users with documentation for usage, test data, expected outputs, and troubleshooting information. Mkdocs is the recommended tool to perform this action, however, other tools may be utilized. The cookiecutter template's (written for mkdocs) provided have basic yaml markdown files provided for this use, and should be edited according to the pipelines function and user needs. </li> </ul>"},{"location":"GitHub/sop_testdata/","title":"GitHub Best Practices: Test Data","text":"<p>The following information is meant to outline test_data requirements for all pipelines, however, should be altered to fit the needs of the specific pipeline developed.</p>"},{"location":"GitHub/sop_testdata/#requirements","title":"Requirements","text":""},{"location":"GitHub/sop_testdata/#readme","title":"README","text":""},{"location":"GitHub/sop_testdata/#partial-test-data","title":"Partial, test data","text":""},{"location":"GitHub/sop_testdata/#complete-test-data","title":"Complete, test data","text":""},{"location":"GitHub/sop_testdata/#configuration-manifest-files","title":"Configuration, manifest files","text":""},{"location":"HPCDME/setup/","title":"Setup","text":""},{"location":"HPCDME/setup/#background","title":"Background","text":"<p>HPC_DME_APIs provides command line utilities or CLUs to interface with HPCDME. This document describes some of the initial setup steps to get the CLUs working on Biowulf.</p>"},{"location":"HPCDME/setup/#setup-steps","title":"Setup steps:","text":""},{"location":"HPCDME/setup/#clone-repo","title":"Clone repo:","text":"<p>The repo can be cloned at a location accessible to you:</p> <pre><code>cd /data/$USER/\ngit clone https://github.com/CBIIT/HPC_DME_APIs.git\n</code></pre>"},{"location":"HPCDME/setup/#create-dirs-log-files-needed-for-hpcmde","title":"Create dirs, log files needed for HPCMDE","text":"<pre><code>mkdir -p /data/$USER/HPCDMELOG/tmp\ntouch /data/$USER/HPCDMELOG/tmp/hpc-cli.log\n</code></pre>"},{"location":"HPCDME/setup/#copy-properties-template","title":"Copy properties template","text":"<p><code>hpcdme.properties</code> is the file that all CLUs look into for various parameters like authentication password, file size limits, number of CPUs, etc. Make a copy of the template provided and prepare it for customization.</p> <pre><code>cd /data/$USER/HPC_DME_APIs/utils\ncp hpcdme.properties-sample hpcdme.properties\n</code></pre>"},{"location":"HPCDME/setup/#customize-properties-file","title":"Customize properties file","text":"<p>Some of the parameters in this file have become obsolete over the course of time and are commmented out. Change paths and default values, as needed</p> <pre><code>#HPC DME Server URL\n#Production server settings\nhpc.server.url=https://hpcdmeapi.nci.nih.gov:8080\nhpc.ssl.keystore.path=hpc-client/keystore/keystore-prod.jks\n#hpc.ssl.keystore.password=hpcdmncif\nhpc.ssl.keystore.password=changeit\n\n#UAT server settings\n#hpc.server.url=https://fr-s-hpcdm-uat-p.ncifcrf.gov:7738/hpc-server\n#hpc.ssl.keystore.path=hpc-client/keystore/keystore-uat.jks\n#hpc.ssl.keystore.password=hpc-server-store-pwd\n\n#Proxy Settings\nhpc.server.proxy.url=10.1.200.240\nhpc.server.proxy.port=3128\n\nhpc.user=$USER\n\n#Globus settings\n#default globus endpoint to be used in registration and download\nhpc.globus.user=$USER\nhpc.default.globus.endpoint=ea6c8fd6-4810-11e8-8ee3-0a6d4e044368\n\n#Log files directory\nhpc.error-log.dir=/data/$USER/HPCDMELOG/tmp\n\n###HPC CLI Logging START####\n#ERROR, WARN, INFO, DEBUG\nhpc.log.level=ERROR\nhpc.log.file=/data/$USER/HPCDMELOG/tmp/hpc-cli.log\n###HPC CLI Logging END####\n\n#############################################################################\n# Please use caution changing following properties. They don't change usually\n#############################################################################\n#hpc.collection.service=collection\n#hpc.dataobject.service=dataObject\n#Log files directory\n#hpc.error-log.dir=.\n\n#Number of thread to run data file import from a CSV file\nhpc.job.thread.count=1\n\nupload.buffer.size=10000000\n\n#Retry count and backoff period for registerFromFilePath (Fixed backoff)\nhpc.retry.max.attempts=3\n#hpc.retry.backoff.period=5000\n\n#Multi-part upload thread pool, threshold and part size configuration\n#hpc.multipart.threadpoolsize=10\n#hpc.multipart.threshold=1074790400\n#hpc.multipart.chunksize=1073741824\n\n#globus.nexus.url=nexus.api.globusonline.org\n#globus.url=www.globusonline.org\n\n#HPC DME Login token file location\nhpc.login.token=tokens/hpcdme-auth.txt\n\n#Globus Login token file location\n#hpc.globus.login.token=tokens/globus-auth.txt\n#validate.md5.checksum=false\n\n# JAR version\n#hpc.jar.version=hpc-cli-1.4.0.jar\n</code></pre> <p>NOTE: The current java version used is: <code>bash java -version openjdk version \"1.8.0_181\" OpenJDK Runtime Environment (build 1.8.0_181-b13) OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)</code></p>"},{"location":"HPCDME/setup/#edit-bashrc","title":"Edit <code>~/.bashrc</code>","text":"<p>Add the CLUs to PATH by adding the following to <code>~/.bashrc</code> file</p> <pre><code># export environment variable HPC_DM_UTILS pointing to directory where \n# HPC DME client utilities are, then source functions script in there \nexport HPC_DM_UTILS=/data/$USER/HPC_DME_APIs/utils\nsource $HPC_DM_UTILS/functions\n</code></pre> <p>Next, source it</p> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"HPCDME/setup/#generate-token","title":"Generate token","text":"<p>Now, you are all set to generate a token. This prevents from re-entering your password everytime.</p> <pre><code>dm_generate_token\n</code></pre> <p>If the token generation takes longer than 45 seconds, check the connection: <pre><code>ping hpcdmeapi.nci.nih.gov\n</code></pre></p> <p>If the connection responds, try to export the following proxy, and then re-run the <code>dm_generate_tokens command</code>: <pre><code>export https_proxy=http://dtn01-e0:3128\n</code></pre></p> <p>Done! You are now all set to use CLUs.</p>"},{"location":"HPCDME/setup/#references-and-links","title":"References and Links","text":"<ul> <li>HPC_DME_APIs repo</li> <li>User guides</li> <li>Wiki pages</li> <li>Yuri Dinh</li> </ul>"},{"location":"HPCDME/transfer/","title":"Biowulf2HPCDME","text":""},{"location":"HPCDME/transfer/#background","title":"Background","text":"<p>Rawdata or Project folders from Biowulf can be parked at a secure location after the analysis has reached a endpoint. Traditionally, CCBR analysts had access to the GridFTP Globus Archive for doing this. But, this Globus Archive has been running past 95% full lately.</p> <p>This document outlines how a projects folder can be directly parked on HPCDME as a single \"tar.gz\" ball. It is assumed that HPC DME API CLUs are already setup as per these instructions.</p> <p>Here are the steps:</p>"},{"location":"HPCDME/transfer/#create-tarball","title":"Create tarball","text":"<p>Once you have a list of files that you want to include in the tarball, create the tarball. This may take a while and should be submitted as a slurm job.</p> <pre><code>% cd /data/CCBR/projects\n% du -hs /data/CCBR/projects/ccbr796\n440G    /data/CCBR/projects/ccbr796\n% echo \"tar czvf ccbr796.tar.gz /data/CCBR/projects/ccbr796\" &gt; do_tar_gz\n% swarm -f do_tar_gz --partition=ccr,norm --time=24:00:00 -t 2 -g 100\n41985209\n</code></pre>"},{"location":"HPCDME/transfer/#create-filelist","title":"Create filelist","text":"<p>Sometime you just want to know what files are in the tarball. Hence, it is important to upload a filelist along with the tarball.</p> <pre><code>% tar tzvf ccbr796.tar.gz &gt; ccbr796.tar.gz.filelist\n</code></pre>"},{"location":"HPCDME/transfer/#create-project","title":"Create Project","text":"<p>If the Project collection does not exist in HPCDME (verify using the web interface), then you may need to create it.</p> <pre><code>% cd /data/kopardevn/SandBox/parkit\n% bash create_empty_project_collection.sh /CCBR_Archive/GRIDFTP/Project_CCBR-796 CCBR-796 CCBR-796\n{\n\"metadataEntries\": [\n{\n\"attribute\": \"collection_type\",\n         \"value\": \"Project\"\n},\n        {\n\"attribute\": \"project_start_date\",\n         \"value\": \"20220616\",\n         \"dateFormat\": \"yyyyMMdd\"\n},\n        {\n\"attribute\": \"access\",\n         \"value\": \"Open Access\"\n},\n        {\n\"attribute\": \"method\",\n         \"value\": \"NGS\"\n},\n        {\n\"attribute\": \"origin\",\n         \"value\": \"CCBR\"\n},\n        {\n\"attribute\": \"project_affiliation\",\n         \"value\": \"CCBR\"\n},\n        {\n\"attribute\": \"project_description\",\n         \"value\": \"CCBR-796\"\n},\n        {\n\"attribute\": \"project_status\",\n         \"value\": \"Completed\"\n},\n        {\n\"attribute\": \"retention_years\",\n         \"value\": \"7\"\n},\n        {\n\"attribute\": \"project_title\",\n         \"value\": \"CCBR-796\"\n},\n        {\n\"attribute\": \"summary_of_samples\",\n         \"value\": \"Unknown\"\n},\n        {\n\"attribute\": \"organism\",\n         \"value\": \"Unknown\"\n}\n]\n}\ndm_register_collection /dev/shm/Project_CCBR-796.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-796\n</code></pre> <p>Error: Have encountered this error message: <pre><code>Error during registration, HTTP_CODE: 503\nCannot find the response message file\ncollection-registration-response-message.json.tmp\n</code></pre> 503 error means that the API is down!</p>"},{"location":"HPCDME/transfer/#create-analysis","title":"Create Analysis","text":"<p>If the Analysis collection does not exist in HPCDME (verify using the web interface) under the Project collection, then you may need to create it.</p> <pre><code>% cd /data/kopardevn/SandBox/parkit\n% bash create_empty_analysis_collection.sh /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis\ndm_register_collection /dev/shm/Analysis.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis\n</code></pre> <p>Done! Now you have a location ready for the the tarball to be parked.</p>"},{"location":"HPCDME/transfer/#create-metadata","title":"Create metadata","text":"<p>Using <code>meta</code> script from <code>pyrkit</code>, we can then generate the required <code>.metadata.json</code> file for the tarball. Analysis collection <code>/CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis</code> should already exist in the HPCDME vault.</p> <pre><code>% echo \"/data/kopardevn/SandBox/pyrkit/src/meta combined --input /data/CCBR/projects/ccbr796.tar.gz --output /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis\" &gt; do_get_metadata\n% swarm -f do_get_metadata --partition=ccr,norm --time=24:00:00 -t 2 -g 100\n</code></pre> <p>If the file is large (100s of GB), then this may take a while as the md5sum of the file is being calculated. Hence, this should be submitted to the slurm. </p> <p>Metadata also needs to be created for the filelist file. These files are generally small (few MBs) and the following command can be directly run on an interactive node.</p> <pre><code>% /data/kopardevn/SandBox/pyrkit/src/meta combined --input /data/CCBR/projects/ccbr796.tar.gz.filelist --output /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis\n</code></pre> <p>The above command, when run sucessfully, will create <code>/data/CCBR/projects/ccbr796.tar.gz.filelist.metadata.json</code></p>"},{"location":"HPCDME/transfer/#transfer","title":"Transfer","text":"<p><code>dm_register_dataobject</code> cannot be used for large files (&gt;10GB), but it can be replaced by <code>dm_register_dataobject_multipart</code></p> <pre><code>% dm_register_dataobject_multipart /data/CCBR/projects/ccbr796.tar.gz.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis/ccbr796.tar.gz /data/CCBR/projects/ccbr796.tar.gz\nReading properties from /data/kopardevn/SandBox/HPC_DME_APIs/utils/hpcdme.properties\nRegistering file: /data/CCBR/projects/ccbr796.tar.gz\nDestination archive path: /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis/ccbr796.tar.gz\nCmd process Completed\nJun 16, 2022 10:55:25 AM org.springframework.shell.core.AbstractShell handleExecutionResult\nINFO: CLI_SUCCESS\n</code></pre> <p>Depending on the size of the file, this step can be done in a reasonable amount of time (&lt;1hr) and can be run in an interactive node.</p> <p>Remember, the filelist file also needs to be registered separately like this:</p> <pre><code>% dm_register_dataobject /data/CCBR/projects/ccbr796.tar.gz.filelist.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis/ccbr796.tar.gz.filelist /data/CCBR/projects/ccbr796.tar.gz.filelist\n</code></pre> <p>As filelist files are smaller (few MBs), we can run <code>dm_register_dataobject</code> in place of <code>dm_register_dataobject_multipart</code>.</p>"},{"location":"HPCDME/transfer/#cleanup","title":"Cleanup","text":"<p>Once the tarball is successfully transferred over the HPCDME, it can be deleted from the local filesystem.</p> <pre><code>% rm -f /data/CCBR/projects/ccbr796.tar.gz\n</code></pre> <p>The contents of the local analysis folder can also be deleted.</p> <pre><code>% cd /data/CCBR/projects/ccbr796 &amp;&amp; rm -rf *\n</code></pre> <p>A note can be added to the recently emptied folder stating where the contents are currently parked.</p> <pre><code>% cd /data/CCBR/projects/ccbr796\n% echo \"This folder was converted to a tarball (tar.gz) and pushed to HPCDME. Its new location is \\`/CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis/ccbr796.tar.gz\\` in HPCDME\" &gt; README.md\n</code></pre> <p>Done!</p> <p>NOTE FOR LARGE FILES: It is recommended by HPCDME staff that files to be transferred should not be too large (1TB or smaller). Hence, if the file to be transferred is larger than 1TB, it should be split into 1TB-size chunks and upload those. Splitting can be done like this: <pre><code>split -b 1T ccbr796.tar.gz \"ccbr796.tar.gz.part_\"\n</code></pre> Metadata needs to be individidually created for each of the parts, namely, <code>ccbr796.tar.gz.part_aa</code>, <code>ccbr796.tar.gz.part_ab</code>, etc  <pre><code>% /data/kopardevn/SandBox/pyrkit/src/meta combined --input /data/CCBR/projects/ccbr796.tar.gz.part_aa --output /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis\n% /data/kopardevn/SandBox/pyrkit/src/meta combined --input /data/CCBR/projects/ccbr796.tar.gz.part_ab --output /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis\n...\n</code></pre> Next each 1TB part can be registered like so: <pre><code>% dm_register_dataobject_multipart /data/CCBR/projects/ccbr796.tar.gz.part_aa.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis/ccbr796.tar.gz.part_aa /data/CCBR/projects/ccbr796.tar.gz.part_aa\n% dm_register_dataobject_multipart /data/CCBR/projects/ccbr796.tar.gz.part_ab.metadata.json /CCBR_Archive/GRIDFTP/Project_CCBR-796/Analysis/ccbr796.tar.gz.part_ab /data/CCBR/projects/ccbr796.tar.gz.part_ab\n...\n</code></pre> Once these files are downloaded then can be joined together using the <code>cat</code> command before ungzipping/untaring. <pre><code>% cat ccbr796.tar.gz.part_* &gt; ccbr796.tar.gz\n</code></pre></p>"},{"location":"Tutorials/summary/","title":"Snakemake","text":"<p>Step-by-step guide for setting up and learning to use Snakemake, with examples and use cases</p> <ul> <li>https://CCBR.github.io/snakemake_tutorial/</li> </ul>"},{"location":"UCSC/creating_inputs/","title":"Creating Inputs","text":""},{"location":"UCSC/creating_inputs/#generating-inputs","title":"Generating Inputs","text":"<p>In order to use the genomic broswer features, sample files must be created.</p>"},{"location":"UCSC/creating_inputs/#individual-sample-files","title":"Individual sample files","text":"<p>For individual samples, where peak density is to be observed, bigwig formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/bigwig). If not using this pipeline, example code is provided below for the file generation. <pre><code>modue load ucsc\n\nfragments_bed=\"/path/to/sample1.fragments.bed\"\nbw=\"/path/to/sample1.bigwig\"\ngenome_len=\"numeric_genome_length\"\nbg=\"/path/to/sample1.bedgraph\"\nbw=\"/path/to/sample2.bigwig\"\n\n# if using a spike-in scale, the scaling factor should be applied\n# while not required, it is recommended for CUT&amp;RUN experiements\nspikein_scale=\"spike_in_value\"\n\n# create bed file\nbedtools genomecov -bg -scale $spikein_scale -i $fragments_bed -g $genome_len &gt; $bg\n\n# create bigwig file\nbedGraphToBigWig $bg $genome_len $bw\n</code></pre></p>"},{"location":"UCSC/creating_inputs/#contrasts-between-samples","title":"Contrasts between samples","text":"<p>For contrasts, where peak differences are to be observed, bigbed formatted files must be generated. If using the CCBR/CARLISLE pipeline these are automatically generated as outputs of the pipeline (WORKDIR/results/peaks/contrasts/contrast_id/). If not using this pipeline, example code is provided below for the file generation. <pre><code>module load ucsc\n\nbed=\"/path/to/sample1_vs_sample2_fragmentsbased_diffresults.bed\"\nbigbed=\"/path/to/output/sample1_vs_sample2_fragmentsbased_diffresults.bigbed\"\ngenome_len=\"numeric_genome_length\"\n\n# create bigbed file\nbedToBigBed -type=bed9 $bed $genome_len $bigbed\n</code></pre></p>"},{"location":"UCSC/creating_inputs/#sharing-data","title":"Sharing data","text":"<p>For all sample types, data must be stored on a shared directory. It is recommended that symlnks be created from the source location to this shared directory to ensure that minial disc space is being used. Example code for creating symlinks is provided below.</p>"},{"location":"UCSC/creating_inputs/#single-sample","title":"single sample","text":"<pre><code># single sample\n## set source file location\nsource_loc=\"/WORKDIR/results/bigwig/sample1.bigwig \"\n\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigwig/sample1.bigwig\"\n\n## create hard links\nln $source_loc $link_loc\n</code></pre>"},{"location":"UCSC/creating_inputs/#contrast-sample","title":"contrast sample","text":"<pre><code># contrast\n## set source file location\nsource_loc=\"WORKDIR/results/peaks/contrasts/sample1_vs_sample2/sample1_vs_sample2_fragmentsbased_diffresults.bigbed \"\n\n## set destination link location\nlink_loc=\"/SHAREDDIR/bigbed/sample1_vs_sample2.bigbed\"\n\n## create hard links\nln $source_loc $link_loc\n</code></pre> <p>Once the links have been generated, the data folder must be open to read and write access.  <pre><code>## set destination link location\nlink_loc=\"/SHAREDDIR/bigbed/\"\n\n# open dir\nchmod -R 777 $link_loc\n</code></pre></p>"},{"location":"UCSC/creating_track_info/","title":"Generating Track information","text":""},{"location":"UCSC/creating_track_info/#single-samples","title":"Single samples","text":"<p>It's recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each sample which will provide the track location, sample name, description of the sample, whether to autoscale the samples, max height of the samples, view limits, and color. An example is provided below.</p> <pre><code>track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\n</code></pre> <p>Users may find it helpful to create a single script which would create this text file for all samples. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.</p>"},{"location":"UCSC/creating_track_info/#inputs","title":"Inputs","text":"<ul> <li>samples_list.txt: a single column text file with sampleID's</li> <li>track_dir: path to the linked files</li> <li>track_output: path to output file</li> <li>peak_list: all peak types to be included</li> <li>method_list: what method to be included</li> <li>dedupe_list: type of duplication to be included</li> </ul> <pre><code># input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a sample_list &lt; $sample_list_input\n\nrun_sample_tracks (){\n    sample_id=$1\n    dedup_id=$2\n\n    # sample name\n    # eg siNC_H3K27Ac_1.dedup.bigwig\n    complete_sample_id=\"${sample_id}.${dedup_id}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigwig/${complete_sample_id}.bigwig\"\n\n    # echo track info\n    echo \"track type=bigWig bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigwig/${complete_sample_id}.bigwig name=${sample_id} description=${sample_id} visibility=full autoScale=off maxHeightPixels=128:30:1 viewLimits=1:120 color=65,105,225\" &gt;&gt; $track_output\n}\n\n# iterate through samples\n# at the sample level only DEDUP matters\nfor sample_id in ${sample_list[@]}; do\n    for dedup_id in ${dedup_list[@]}; do\n        run_sample_tracks $sample_id $dedup_id\n    done\ndone\n</code></pre>"},{"location":"UCSC/creating_track_info/#contrast-samples","title":"Contrast samples","text":"<p>It's recommended to create a text file of all sample track information to ease in editing and submission to the UCSC browser website. A single line of code is needed for each contrast which will provide the track location, contrast name, file type, and whether to color the sample. An example is provided below.</p> <pre><code>track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On\n</code></pre> <p>Users may find it helpful to create a single script which would create this text file for all contrasts. An example of this is listed below, which assumes that input files were generated using the CARLISLE pipeline. It can be edited to adapt to other output files, as needed.</p>"},{"location":"UCSC/creating_track_info/#inputs_1","title":"Inputs","text":"<ul> <li>samples_list.txt: a single column text file with sampleID's</li> <li>track_dir: path to the linked files</li> <li>track_output: path to output file</li> <li>peak_list: all peak types to be included</li> <li>method_list: what method to be included</li> <li>dedupe_list: type of duplication to be included</li> </ul> <pre><code># input arguments\nsample_list_input=/\"path/to/samples.txt\"\ntrack_dir=\"/path/to/shared/dir/\"\ntrack_output=\"/path/to/output/file/tracks.txt\npeak_list=(\"norm.relaxed.bed\" \"narrowPeak\" \"broadGo_peaks.bed\" \"narrowGo_peaks.bed\")\nmethod_list=(\"fragmentsbased\")\ndedup_list=(\"dedup\")\n\n# read sample file\nIFS=$'\\n' read -d '' -r -a deg_list &lt; $deg_list_input\n\nrun_comparison_tracks (){\n    peak_type=$1\n    method_type=$2\n    dedup_type=$3\n    sample_id=$4\n\n    # sample name\n    # eg siSmyd3_2m_Smyd3_0.25HCHO_500K_vs_siNC_2m_Smyd3_0.25HCHO_500K__no_dedup__norm.relaxed\n    complete_sample_id=\"${sample_id}__${dedup_type}__${peak_type}\"\n\n    # set link location\n    link_loc=\"${track_dir}/bigbed/${complete_sample_id}_${method_type}_diffresults.bigbed\"\n\n    # echo track info\n    echo \"track name=${sample_id}_${peak_type} bigDataUrl=https://hpc.nih.gov/~CCBR/ccbr1155/${dir_loc}/bigbed/${complete_sample_id}_fragmentsbased_diffresults.bigbed type=bigBed itemRgb=On\" &gt;&gt; $track_info\n}\n\n# iterate through samples / peaks / methods / dedup\nfor sample_id in ${deg_list[@]}; do\n    for peak_id in ${peak_list[@]}; do\n        for method_id in ${method_list[@]}; do\n            for dedup_id in ${dedup_list[@]}; do\n                run_comparison_tracks $peak_id $method_id $dedup_id $sample_id\n            done\n        done\n    done\ndone\n</code></pre>"},{"location":"UCSC/creating_track_info/#other-tips","title":"Other tips","text":"<p>Users can also change the colors of the tracks using standard HTML color features. Common colors used are provided below: <pre><code>Red=205,92,92\nBlue=65,105,225\nBlack=0,0,0\n</code></pre></p>"},{"location":"UCSC/creating_tracks/","title":"Generating New Tracks","text":"<ol> <li>Login to VPN</li> <li>Login to the UCSC Browser website</li> <li>Select \"My Data &gt; Custom Tracks\"</li> <li>Select \"Add Custom Tracks\"</li> <li>Paste the track data generated in Creating Track Info into the text box</li> <li>Select \"Submit\"</li> <li>Review the track information. The column \"Error\" should be empty. If there is an error then a hyperlink will display \"Show\" although this often does not contain helpful error information.</li> <li>After troubleshooting any errors select \"Go\"</li> <li>Use the features at the bottom of the page to alter views and/or add additional track information</li> <li>Select \"My Data &gt; My Sessions\"</li> <li>Under \"Save current settings as named session:\" enter a descriptive name of the session</li> <li>Select \"Submit\"</li> <li>This will move the descriptive name entered into the \"session name\" list</li> <li>Select the descriptve name, view your track information as saved</li> <li>Copy the hyperlink for this session and share as needed</li> </ol>"},{"location":"UCSC/creating_tracks/#editing-a-previous-tracks","title":"Editing a Previous Tracks","text":"<ol> <li>Login to VPN</li> <li>Login to the UCSC Browser website</li> <li>Select \"My Data &gt; My Sessions\"</li> <li>Select the descriptve name of the session you'd like to edit</li> <li>Edit the tracks as needed:<ul> <li>If wanting to remove or add tracks, then select \"My Data &gt; Custom Tracks\"; follow steps 5-8 above.</li> <li>If wanting to edit the view of the tracks, follow step 9 above.</li> </ul> </li> <li>Select \"My Data &gt; My Sessions\"</li> <li>Under \"Save current settings as named session:\" enter a new descriptive name OR the previous name of the session if you'd like to overwrite it</li> <li>Select \"Submit\"</li> <li>This will move the descriptive name entered into the \"session name\" list</li> <li>Select the descriptve name, view your track information as saved</li> <li>Copy the hyperlink for this session and share as needed</li> </ol>"},{"location":"UCSC/overview/","title":"Overview","text":"<p>The UCSC Genome Browser allows for visualization of genomic data in an interactive and shareable format. User's must create accounts with their NIH credentials, and have an active Biowulf account to create the tracks. In addition users have to be connect to VPN in order to view and create the tracks. Once bigwig files are generated and stored in a shared data location, genomic tracks can be edited, and permanent links created, accessible for collaborators to view.</p>"}]}